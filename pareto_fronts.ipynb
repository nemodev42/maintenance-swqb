{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinforcment Learning with Foirier/Polynomial State Weighted Q Basis for Homogeus Multi Component Maitenance\n",
    "# By Joseph Wittrock\n",
    "\n",
    "# Enviroment/Training code adapted from:\n",
    "# https://pytorch.org/rl/stable/tutorials/torchrl_envs.html\n",
    "# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "# The enviroment is composed of n components with m condition states. Each component degrades each step according to its degredation transition matrix. \n",
    "# The actions to the enviroment are 0 for do nothing, or 1 for repair.\n",
    "# Only k components can be repaired at a time, and the repair cost is proportional to the number of components repaired.\n",
    "# If a component is in a failed state, it will have a failure cost no matter what action is taken. (Though I want to change how this works for multiperiod adaptation)\n",
    "# The goal is to maximize the reward over a fixed number of steps, reward is negative for repair costs and failure costs.\n",
    "\n",
    "# The enviroment is written using TorchRL and TensorDict for efficient paralell computation on a cuda enabled GPU.\n",
    "\n",
    "# The state space is reduced by considering the distrobution of the components in each condition state, rather than the condition state of each component.\n",
    "# This condenses the state space to m states.\n",
    "\n",
    "# To generalize the action space we search for an \"equitable\" policy. i.e. a component in worse condition is always repaired before a component in better condition.\n",
    "# This reduces the action space to k actions.\n",
    "\n",
    "# For large values of m, a traditional deep Q network would not generalize well, as there are too many output neurons.\n",
    "# Instead, we use a basis of continuous valued functions over [0,1], and map the action space to i \\mapsto i/k for i in [0,k]. \n",
    "# Note when i=0, no components are repaired, and when i=k, all components are repaired, so there are k+1 actions in total.\n",
    "\n",
    "# The degrees of approximation determine the output nodes for the neural network, then the output is dot producted with the basis functions evaluated at the precalculated action domain points.\n",
    "# This allows for scale free computation for increasing values of the repair constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from matplotlib import pyplot as plt # for plotting\n",
    "import numpy as np # for cpu based computation\n",
    "import torch # for efficient (gpu) computation and automatic differentiation\n",
    "from tqdm import tqdm # for progress bars\n",
    "from tensordict import TensorDict, TensorDictBase # for handling dictionaries of tensors in a pytorch friendly way, e.g. for batched data\n",
    "from torch import nn # for neural networks\n",
    "import torch.optim as optim # for optimizers\n",
    "import torch.nn.functional as F # for activation functions\n",
    "from torch.utils.tensorboard import SummaryWriter # for logging to tensorboard\n",
    "\n",
    "\n",
    "# TorchRL\n",
    "from torchrl.data import BoundedTensorSpec, CompositeSpec, UnboundedContinuousTensorSpec # for defining the shape and type of data [Legacy]\n",
    "from torchrl.data import Bounded, Composite, Unbounded # for defining the shape and type of data\n",
    "from torchrl.envs import (\n",
    "    CatTensors, # Concatenate tensors\n",
    "    EnvBase, # Tensordict based env\n",
    "    Transform, # Transform for envs\n",
    ")\n",
    "from torchrl.envs.transforms.transforms import _apply_to_composite # for applying a transform to a composite spec\n",
    "from torchrl.envs.utils import check_env_specs, step_mdp # for checking env specs and stepping through an MDP\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # enable cuda if available \n",
    "print(f\"Using device: {device}\")\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "#                    #\n",
    "#   HYPERPARAMETERS  #\n",
    "#                    #\n",
    "######################\n",
    "\n",
    "# Environment parameters\n",
    "N_COMPONENTS = 1000 # number of components in each enviroment\n",
    "MAX_REPAIR_CONSTRAINT = 50 # Maximum number of components that can be repaired in a single step\n",
    "EPISODE_LENGTH = 128 # number of steps in each episode\n",
    "STEP_BATCH_SIZE = 128 # number of environments to be executed in paralell\n",
    "# - Cost parameters\n",
    "# REPAIR_COST = 10.0 # cost per repair action taken each step\n",
    "# FAILURE_COST = 20.0 # cost per maximum state (failed state) component each step\n",
    "REWARD_BLEND = 0.8 # weight for blending orm costs and utility\n",
    "\n",
    "# Optimization parameters\n",
    "OPTIMIZATION_BATCH_SIZE = 64 * EPISODE_LENGTH # number of transitions to be used used in each optimization batch\n",
    "BUFFER_SIZE = 1 * STEP_BATCH_SIZE * EPISODE_LENGTH # number of transitions to be stored in the memory replay buffer\n",
    "OPTIMIZATION_PASSES = 1 # number of optimization passes to be made in each step\n",
    "GAMMA = 0.98 # reward discount factor \\in [0, 1]\n",
    "TAU = 1e-2 # soft target network learning rate. \\in [0, 1]\n",
    "LR = 1e-2 # learning rate for policy network. \\in [0, 1]\n",
    "N_EPISODES = 100\n",
    "EPOCHS = N_EPISODES * EPISODE_LENGTH  # number of epochs to train the policy network\n",
    "# - Entropy parameters\n",
    "ENTROPY_START = 0.5 # initial entropy value for random actions\n",
    "ENTROPY_END = 0.05  # entropy value for random actions\n",
    "ENTROPY_DECAY = 1_000  # entropy decay rate \n",
    "# - Agent Network parameters\n",
    "BASIS =   \"polynomial\" # \"fourier\" #\n",
    "N_DEEP_NODES = 32 # number of nodes in the deep network\n",
    "DEGREE_APPROXIMATION = 3 # basis elements to be generated for action value approximation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Generate Other Parameters ######\n",
    "\n",
    "# evaluate basis functions at every relevant action\n",
    "# a = k / MAX_REPAIR_CONSTRAINT \\in [0, 1] \n",
    "# for k \\in { 0, 1, 2, ...,  MAX_REPAIR_CONSTRAINT }\n",
    "basis_switch = { \n",
    "    \"polynomial\": torch.tensor([\n",
    "        [ (n / MAX_REPAIR_CONSTRAINT )**k for n in range(MAX_REPAIR_CONSTRAINT + 1) ] \n",
    "        for k in range(DEGREE_APPROXIMATION)\n",
    "    ], dtype=torch.float32),\n",
    "    \"fourier\": torch.tensor([\n",
    "        [np.cos(1 * np.pi * k * n / MAX_REPAIR_CONSTRAINT) for n in range(MAX_REPAIR_CONSTRAINT + 1)]\n",
    "        for k in range(DEGREE_APPROXIMATION)\n",
    "    ], dtype=torch.float32),\n",
    "    }\n",
    "basis = basis_switch[ BASIS ]\n",
    "\n",
    "\n",
    "def get_basis(degree: int, max_repair_constraint: int, basis_type: str = \"polynomial\") -> torch.Tensor:\n",
    "    if basis_type == \"polynomial\":\n",
    "        return torch.tensor([\n",
    "            [ (n / max_repair_constraint )**k for n in range(max_repair_constraint + 1) ] \n",
    "            for k in range(degree)\n",
    "        ], dtype=torch.float32)\n",
    "    elif basis_type == \"fourier\":\n",
    "        return torch.tensor([\n",
    "            [np.cos(1 * np.pi * k * n / max_repair_constraint) for n in range(max_repair_constraint + 1)]\n",
    "            for k in range(degree)\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "basis = basis.to(device)\n",
    "basis.shape\n",
    "\n",
    "# Degredation Transition Matrix\n",
    "# Degredation Transition Matrix\n",
    "transition_matrix_degrade = torch.tensor([\n",
    "    [0.90, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.10, 0.90, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.05, 0.8, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.05, 0.15, 0.8, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.05, 0.1, 0.8, 0.0],\n",
    "    [0.0, 0.00, 0.00, 0.1, 0.2, 1.0],\n",
    "], device=device).T \n",
    "\n",
    "# Number of discrete condition states per component\n",
    "N_CONDITION_STATES = transition_matrix_degrade.shape[0]\n",
    "\n",
    "# Repair Matrix\n",
    "transition_matrix_repair = torch.zeros((N_CONDITION_STATES, N_CONDITION_STATES), device=device)\n",
    "transition_matrix_repair[:, 0] = 1.0 # send to the first state\n",
    "\n",
    "# Pack matrices into a single tensor\n",
    "transition_matrices = torch.stack([transition_matrix_degrade, transition_matrix_repair])\n",
    "\n",
    "# Reward function\n",
    "def make_r_s_a(repair_cost, failure_cost):\n",
    "    r_s_a = torch.zeros(N_CONDITION_STATES, 2, device=device)\n",
    "    r_s_a[:, 1] = -repair_cost\n",
    "    r_s_a[-1, :] = -failure_cost\n",
    "    return r_s_a\n",
    "\n",
    "# r_s_a = make_r_s_a(REPAIR_COST, FAILURE_COST)\n",
    "\n",
    "\n",
    "# Homogeneous Setup\n",
    "transition_tensor = torch.cat([transition_matrices for _ in range(N_COMPONENTS)]).reshape(N_COMPONENTS, 2, N_CONDITION_STATES, N_CONDITION_STATES)\n",
    "# rsa_tensor = torch.cat([r_s_a for _ in range(N_COMPONENTS)]).reshape(N_COMPONENTS, N_CONDITION_STATES, 2)\n",
    "\n",
    "# Max cost used for normalizing rewards\n",
    "# max_cost = FAILURE_COST * N_COMPONENTS\n",
    "\n",
    "# rsa_tensor /= max_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter dictionary for accessing hyperparameters in other scripts\n",
    "\n",
    "hyperparameters = TensorDict(\n",
    "    N_COMPONENTS=N_COMPONENTS,\n",
    "    N_CONDITION_STATES=N_CONDITION_STATES,\n",
    "    MAX_REPAIR_CONSTRAINT=MAX_REPAIR_CONSTRAINT,\n",
    "    EPISODE_LENGTH=EPISODE_LENGTH,\n",
    "    STEP_BATCH_SIZE=STEP_BATCH_SIZE,\n",
    "    # REPAIR_COST=REPAIR_COST,\n",
    "    # FAILURE_COST=FAILURE_COST,\n",
    "    REWARD_BLEND=REWARD_BLEND,\n",
    "    OPTIMIZATION_BATCH_SIZE=OPTIMIZATION_BATCH_SIZE,\n",
    "    BUFFER_SIZE=BUFFER_SIZE,\n",
    "    OPTIMIZATION_PASSES=OPTIMIZATION_PASSES,\n",
    "    GAMMA=GAMMA,\n",
    "    TAU=TAU,\n",
    "    LR=LR,\n",
    "    N_EPISODES=N_EPISODES,\n",
    "    EPOCHS=EPOCHS,\n",
    "    ENTROPY_START=ENTROPY_START,\n",
    "    ENTROPY_END=ENTROPY_END,\n",
    "    ENTROPY_DECAY=ENTROPY_DECAY,\n",
    "    BASIS=BASIS,\n",
    "    BASIS_DOMAIN=basis,\n",
    "    N_DEEP_NODES=N_DEEP_NODES,\n",
    "    DEGREE_APPROXIMATION=DEGREE_APPROXIMATION,\n",
    ")\n",
    "\n",
    "hyperparameters.to(device)\n",
    "\n",
    "# hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maitenance_util_cost_env import DiscreteMaitenanceEnv, gen_params,  load_dynamics, generate_maitenance_env, reset_maitenance_env\n",
    "\n",
    "utility = torch.linspace(0, 1, N_COMPONENTS, device=device) \n",
    "orm_costs = torch.linspace(0, 1, MAX_REPAIR_CONSTRAINT+1, device=device) \n",
    "\n",
    "# load data into enviroment global variables\n",
    "\n",
    "load_dynamics(transition_tensor, orm_costs, utility, hyperparameters)\n",
    "\n",
    "# generate enviroment based on hyperparameters and transition/reward tensors\n",
    "env = generate_maitenance_env(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Hyperparameters:\n",
    "linear_best_lr =  0.013366665691137314\n",
    "# Linear AK Hyperparameters:\n",
    "linear_AK_best_lr =  0.006733333226293325\n",
    "linear_AK_best_degree =  3\n",
    "# Linear AK Fourier Hyperparameters:\n",
    "linear_AK_fourier_best_lr =  0.019170833751559258\n",
    "linear_AK_fourier_best_degree =  8\n",
    "# DQ Hyperparameters:\n",
    "DQ_best_lr =  0.011708333157002926\n",
    "DQ_best_deep =  24\n",
    "# DQAK Hyperparameters:\n",
    "DQAK_best_lr =  0.007562499959021807\n",
    "DQAK_best_degree =  3\n",
    "DQAK_best_deep =  48\n",
    "# DQAK Forier Hyperparameters:\n",
    "DQAK_forier_best_lr =  0.00507500022649765\n",
    "DQAK_forier_best_degree =  5\n",
    "DQAK_forier_best_deep =  128\n",
    "\n",
    "test_observation = torch.tensor([0.4290, 0.1980, 0.1290, 0.1580, 0.0610, 0.0250], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_q_target_training import MaitenanceDQBNTrainer\n",
    "\n",
    "reward_blend_range = torch.linspace(0, 1, 20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear\n",
    "hyperparameters[\"LR\"] = linear_best_lr\n",
    "\n",
    "linear_utility = torch.zeros_like(reward_blend_range)\n",
    "linear_orm_costs = torch.zeros_like(reward_blend_range)\n",
    "\n",
    "for i, reward_blend in enumerate(reward_blend_range):\n",
    "\tprint(\"test:\", i)\n",
    "\thyperparameters[\"REWARD_BLEND\"] = reward_blend\n",
    "\tdqt_linear = MaitenanceDQBNTrainer(hyperparameters, env,)\n",
    "\tdqt_linear.create_linear_models()\n",
    "\tdqt_linear.train()\n",
    "\n",
    "\tutil, cost = dqt_linear.benchmark_UC(episode_length=300)\n",
    "\tlinear_utility[i] = util\n",
    "\tlinear_orm_costs[i] = cost\n",
    "\n",
    "\n",
    "linear_rewards = torch.cat([linear_utility.unsqueeze(0), linear_orm_costs.unsqueeze(0)], dim=0)\n",
    "\n",
    "# save rewards\n",
    "torch.save(linear_rewards, \"linear_rewards.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(linear_orm_costs.cpu().numpy(), linear_utility.cpu().numpy())\n",
    "plt.xlabel(\"Cost\")\n",
    "plt.ylabel(\"Utility\")\n",
    "plt.title(\"Linear Approximation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear SWQB (formely linea AK)\n",
    "hyperparameters[\"LR\"] = linear_AK_best_lr\n",
    "hyperparameters[\"DEGREE_APPROXIMATION\"] = linear_AK_best_degree\n",
    "hyperparameters[\"BASIS_DOMAIN\"] = get_basis(linear_AK_best_degree, MAX_REPAIR_CONSTRAINT, basis_type=\"polynomial\")\n",
    "\n",
    "linear_AK_utility = torch.zeros_like(reward_blend_range)\n",
    "linear_AK_orm_costs = torch.zeros_like(reward_blend_range)\n",
    "\n",
    "for i, reward_blend in enumerate(reward_blend_range):\n",
    "\tprint(\"test:\", i)\n",
    "\thyperparameters[\"REWARD_BLEND\"] = reward_blend\n",
    "\tdqt_linear_AK = MaitenanceDQBNTrainer(hyperparameters, env,)\n",
    "\tdqt_linear_AK.create_linear_AK_models()\n",
    "\tdqt_linear_AK.train()\n",
    "\n",
    "\tutil, cost = dqt_linear_AK.benchmark_UC(episode_length=300)\n",
    "\tlinear_AK_utility[i] = util\n",
    "\tlinear_AK_orm_costs[i] = cost\n",
    "\n",
    "\n",
    "linear_AK_rewards = torch.cat([linear_AK_utility.unsqueeze(0), linear_AK_orm_costs.unsqueeze(0)], dim=0)\n",
    "\n",
    "# save rewards\n",
    "torch.save(linear_AK_rewards, \"linear_AK_rewards.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(linear_orm_costs.cpu().numpy(), linear_utility.cpu().numpy())\n",
    "plt.plot(linear_AK_orm_costs.cpu().numpy(), linear_AK_utility.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear AK Fourier\n",
    "hyperparameters[\"LR\"] = linear_AK_fourier_best_lr\n",
    "hyperparameters[\"DEGREE_APPROXIMATION\"] = linear_AK_fourier_best_degree\n",
    "hyperparameters[\"BASIS_DOMAIN\"] = get_basis(linear_AK_fourier_best_degree, MAX_REPAIR_CONSTRAINT, basis_type=\"fourier\")\n",
    "\n",
    "linear_AK_fourier_utility = torch.zeros_like(reward_blend_range)\n",
    "linear_AK_fourier_orm_costs = torch.zeros_like(reward_blend_range)\n",
    "\n",
    "for i, reward_blend in enumerate(reward_blend_range):\n",
    "\tprint(\"test:\", i)\n",
    "\thyperparameters[\"REWARD_BLEND\"] = reward_blend\n",
    "\tdqt_linear_AK_fourier = MaitenanceDQBNTrainer(hyperparameters, env,)\n",
    "\tdqt_linear_AK_fourier.create_linear_AK_models()\n",
    "\tdqt_linear_AK_fourier.train()\n",
    "\n",
    "\tutil, cost = dqt_linear_AK_fourier.benchmark_UC(episode_length=300)\n",
    "\tlinear_AK_fourier_utility[i] = util\n",
    "\tlinear_AK_fourier_orm_costs[i] = cost\n",
    "\n",
    "\n",
    "linear_AK_fourier_rewards = torch.cat([linear_AK_fourier_utility.unsqueeze(0), linear_AK_fourier_orm_costs.unsqueeze(0)], dim=0)\n",
    "\n",
    "# save rewards\n",
    "torch.save(linear_AK_fourier_rewards, \"linear_AK_fourier_rewards.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(linear_orm_costs.cpu().numpy(), linear_utility.cpu().numpy())\n",
    "plt.plot(linear_AK_orm_costs.cpu().numpy(), linear_AK_utility.cpu().numpy())\n",
    "plt.plot(linear_AK_fourier_orm_costs.cpu().numpy(), linear_AK_fourier_utility.cpu().numpy())\n",
    "plt.xlabel(\"Cost\")\n",
    "plt.ylabel(\"Utility\")\n",
    "plt.title(\"Linear SWQB Polynomial Approximation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12800 [00:00<?, ?it/s]/home/nemo/anaconda3/envs/torch/lib/python3.12/site-packages/torchrl/data/tensor_specs.py:5464: DeprecationWarning: The BoundedTensorSpec has been deprecated and will be removed in v0.7. Please use Bounded instead.\n",
      "  warnings.warn(\n",
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 205.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:01<00:00, 207.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:01<00:00, 207.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:01<00:00, 207.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:01<00:00, 207.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:01<00:00, 207.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:01<00:00, 207.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:01<00:00, 207.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:01<00:00, 206.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 205.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 205.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 205.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 205.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 206.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 205.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 205.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 206.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 205.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:01<00:00, 206.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 205.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Deep Q\n",
    "hyperparameters[\"LR\"] = DQ_best_lr\n",
    "hyperparameters[\"N_DEEP_NODES\"] = DQ_best_deep\n",
    "\n",
    "DQ_utility = torch.zeros_like(reward_blend_range)\n",
    "DQ_orm_costs = torch.zeros_like(reward_blend_range)\n",
    "\n",
    "for i, reward_blend in enumerate(reward_blend_range):\n",
    "\tprint(\"test:\", i)\n",
    "\thyperparameters[\"REWARD_BLEND\"] = reward_blend\n",
    "\tdqt_DQ = MaitenanceDQBNTrainer(hyperparameters, env,)\n",
    "\tdqt_DQ.create_models()\n",
    "\tdqt_DQ.train()\n",
    "\n",
    "\tutil, cost = dqt_DQ.benchmark_UC(episode_length=300)\n",
    "\tDQ_utility[i] = util\n",
    "\tDQ_orm_costs[i] = cost\n",
    "\n",
    "\n",
    "# plt.scatter(DQ_orm_costs.cpu().numpy(), DQ_utility.cpu().numpy())\n",
    "# plt.xlabel(\"Cost\")\n",
    "# plt.ylabel(\"Utility\")\n",
    "# plt.title(\"Deep Q Approximation\")\n",
    "# plt.show()\n",
    "\n",
    "DQ_rewards = torch.cat([DQ_utility.unsqueeze(0), DQ_orm_costs.unsqueeze(0)], dim=0)\n",
    "\n",
    "# save rewards\n",
    "torch.save(DQ_rewards, \"DQ_rewards.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:01<00:00, 207.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:01<00:00, 207.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 206.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 204.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 205.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 204.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 203.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 203.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 203.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:03<00:00, 202.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:03<00:00, 202.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:03<00:00, 202.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:03<00:00, 203.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 203.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:03<00:00, 202.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 203.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:03<00:00, 203.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:03<00:00, 202.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 203.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:03<00:00, 202.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# Deep SWQB\n",
    "hyperparameters[\"LR\"] = DQAK_best_lr\n",
    "hyperparameters[\"DEGREE_APPROXIMATION\"] = DQAK_best_degree\n",
    "hyperparameters[\"N_DEEP_NODES\"] = DQAK_best_deep\n",
    "hyperparameters[\"BASIS_DOMAIN\"] = get_basis(DQAK_best_degree, MAX_REPAIR_CONSTRAINT, basis_type=\"polynomial\")\n",
    "\n",
    "DQAK_utility = torch.zeros_like(reward_blend_range)\n",
    "DQAK_orm_costs = torch.zeros_like(reward_blend_range)\n",
    "\n",
    "for i, reward_blend in enumerate(reward_blend_range):\n",
    "\tprint(\"test:\", i)\n",
    "\thyperparameters[\"REWARD_BLEND\"] = reward_blend\n",
    "\tdqt_DQAK = MaitenanceDQBNTrainer(hyperparameters, env,)\n",
    "\tdqt_DQAK.create_AK_models()\n",
    "\tdqt_DQAK.train()\n",
    "\n",
    "\tutil, cost = dqt_DQAK.benchmark_UC(episode_length=300)\n",
    "\tDQAK_utility[i] = util\n",
    "\tDQAK_orm_costs[i] = cost\n",
    "\n",
    "\n",
    "# plt.scatter(DQAK_orm_costs.cpu().numpy(), DQAK_utility.cpu().numpy())\n",
    "# plt.xlabel(\"Cost\")\n",
    "# plt.ylabel(\"Utility\")\n",
    "# plt.title(\"Deep SWQB Polynomial Approximation\")\n",
    "# plt.show()\n",
    "\n",
    "DQAK_rewards = torch.cat([DQAK_utility.unsqueeze(0), DQAK_orm_costs.unsqueeze(0)], dim=0)\n",
    "\n",
    "# save rewards\n",
    "torch.save(DQAK_rewards, \"DQAK_rewards.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 203.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 203.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 203.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 203.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 203.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 203.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 203.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 203.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 203.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:03<00:00, 202.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:03<00:00, 203.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:02<00:00, 203.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:03<00:00, 202.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:03<00:00, 202.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:03<00:00, 203.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:03<00:00, 201.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:03<00:00, 201.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:03<00:00, 201.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:04<00:00, 198.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy: 0.05: 100%|██████████| 12800/12800 [01:01<00:00, 209.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# Deep SWQB Forier\n",
    "hyperparameters[\"LR\"] = DQAK_forier_best_lr\n",
    "hyperparameters[\"DEGREE_APPROXIMATION\"] = DQAK_forier_best_degree\n",
    "hyperparameters[\"N_DEEP_NODES\"] = DQAK_forier_best_deep\n",
    "hyperparameters[\"BASIS_DOMAIN\"] = get_basis(DQAK_forier_best_degree, MAX_REPAIR_CONSTRAINT, basis_type=\"fourier\")\n",
    "\n",
    "DQAK_forier_utility = torch.zeros_like(reward_blend_range)\n",
    "DQAK_forier_orm_costs = torch.zeros_like(reward_blend_range)\n",
    "\n",
    "for i, reward_blend in enumerate(reward_blend_range):\n",
    "\tprint(\"test:\", i)\n",
    "\thyperparameters[\"REWARD_BLEND\"] = reward_blend\n",
    "\tdqt_DQAK_forier = MaitenanceDQBNTrainer(hyperparameters, env,)\n",
    "\tdqt_DQAK_forier.create_AK_models()\n",
    "\tdqt_DQAK_forier.train()\n",
    "\n",
    "\tutil, cost = dqt_DQAK_forier.benchmark_UC(episode_length=300)\n",
    "\tDQAK_forier_utility[i] = util\n",
    "\tDQAK_forier_orm_costs[i] = cost\n",
    "\n",
    "\n",
    "# plt.scatter(DQAK_forier_orm_costs.cpu().numpy(), DQAK_forier_utility.cpu().numpy())\n",
    "# plt.xlabel(\"Cost\")\n",
    "# plt.ylabel(\"Utility\")\n",
    "# plt.title(\"Deep SWQB Forier Approximation\")\n",
    "# plt.show()\n",
    "\n",
    "DQAK_forier_rewards = torch.cat([DQAK_forier_utility.unsqueeze(0), DQAK_forier_orm_costs.unsqueeze(0)], dim=0)\n",
    "\n",
    "# save rewards\n",
    "torch.save(DQAK_forier_rewards, \"DQAK_forier_rewards.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(linear_orm_costs.cpu().numpy(), linear_utility.cpu().numpy())\n",
    "plt.plot(linear_AK_orm_costs.cpu().numpy(), linear_AK_utility.cpu().numpy())\n",
    "plt.plot(linear_AK_fourier_orm_costs.cpu().numpy(), linear_AK_fourier_utility.cpu().numpy())\n",
    "plt.plot(DQ_orm_costs.cpu().numpy(), DQ_utility.cpu().numpy())\n",
    "plt.plot(DQAK_orm_costs.cpu().numpy(), DQAK_utility.cpu().numpy())\n",
    "plt.plot(DQAK_forier_orm_costs.cpu().numpy(), DQAK_forier_utility.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all pareto fronts\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(linear_orm_costs.cpu().numpy(), linear_utility.cpu().numpy(), label=\"Linear\")\n",
    "plt.scatter(linear_AK_orm_costs.cpu().numpy(), linear_AK_utility.cpu().numpy(), label=\"Linear SWQB\")\n",
    "plt.scatter(DQ_orm_costs.cpu().numpy(), DQ_utility.cpu().numpy(), label=\"Deep Q\")\n",
    "plt.scatter(DQAK_orm_costs.cpu().numpy(), DQAK_utility.cpu().numpy(), label=\"Deep SWQB\")\n",
    "plt.scatter(DQAK_forier_orm_costs.cpu().numpy(), DQAK_forier_utility.cpu().numpy(), label=\"Deep SWQB Forier\")\n",
    "plt.xlabel(\"Cost\")\n",
    "plt.ylabel(\"Utility\")\n",
    "plt.title(\"Pareto Fronts\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all pareto fronts\n",
    "plt.scatter(linear_orm_costs.cpu().numpy(), linear_utility.cpu().numpy(), label=\"Linear\")\n",
    "plt.scatter(linear_AK_orm_costs.cpu().numpy(), linear_AK_utility.cpu().numpy(), label=\"Linear SWQB\")\n",
    "plt.scatter(DQ_orm_costs.cpu().numpy(), DQ_utility.cpu().numpy(), label=\"Deep Q\")\n",
    "plt.scatter(DQAK_orm_costs.cpu().numpy(), DQAK_utility.cpu().numpy(), label=\"Deep SWQB\")\n",
    "plt.scatter(DQAK_forier_orm_costs.cpu().numpy(), DQAK_forier_utility.cpu().numpy(), label=\"Deep SWQB Fourier\")\n",
    "plt.xlabel(\"Cost\")\n",
    "plt.ylabel(\"Utility\")\n",
    "plt.title(\"Pareto Fronts\")\n",
    "plt.legend()\n",
    "plt.ylim(0.5, 1.0)\n",
    "plt.xlim(0.8, 1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
